# About

  Word embeddings are a matrix for some vocabulary of a given language. Each row of this matrix corresponds to a word vector, which is typically less than one hundred, or several hundred components and the size of the vocabulary can be hundreds of thousands large. Word embeddings are very computationally expensive to train and most ML practitioners will load a pre-trained set of embeddings. With word embeddings, we can effectively do lots of things related to natural language (e.g., machine translation, mood classification, etc) we encounter daily with simple machine learning models.  In this project, we built two versions of models for sentence mood and output the reasonable emojis.  In the first version, a simple neuron network (a full layer + a softmax output layer) is constructed with the average vector for the word vectors of a given sentence being the input layer. The second model further takes the order of words into account in a recurrent structure in which two LSTM layers are included. The version of word matrix (glove.6B.50d)[https://www.kaggle.com/watts2/glove6b50dtxt]  is used and other core packages are: (emoji)[https://pypi.org/project/emoji/], (keras)[https://keras.io/].
  
# Run
Before run, download a copy of (glove.6B.50d)[https://www.kaggle.com/watts2/glove6b50dtxt] and put it in the same directory as Emojify.ipynb. Then, run Emojify.ipynb.
